# Section 3. 자연어 처리를 위한 기초 지식

## Tokenizing & One-hot Encoding
### *Tokenizing*
전체 텍스트를 원하는 구분 단위로 나누는 것
  - 전체 텍스트 -> 문장 단위, 전체 문장 -> 단어 단위 또는 형태소 단위까지 토크나이징 할 수 있음

### *One-hot Encoding*
범주형 값을 이진화된 값으로 바꿔서 표현하는 것
  - 범주형 값 : 개, 고양이, 말이라는 3개의 범주형 데이터를 [개=1, 고양이=2, 말=3]이라고 단순하게 Integer Label Encoding으로 변환하여 표현하는 것
    - 문제점 : 머신러닝 알고리즘이 정수 값으로부터 잘못된 경향성을 학습하게 될 수 있음. 예를 들어 위의 예시에서 머신러닝 알고리즘이 개(=1)와 말(=3)의 평균은 고양이(=2)라는 잘못된 지식을 학습할 수 있음.
  - One-hot Encoding : 범주형 데이터를 개=[1 0 0], 고양이=[0 1 0], 말=[0 0 1]과 같이 해당 레이블을 나타내는 인덱스만 1의 값을 가지고 나머지 부분은 0의 값을 가진 Binary Value로 표현
  > 머신 러닝 알고리즘을 구현할 때 타겟 데이터를 One-hot Encoding 형태로 표현하는 것이 일반적임.
  - 전통적인 자연어 처리 방법론에서는 단어 하나를 원핫 인코딩 형태로 표현
  - 이때, 원핫 인코딩 벡터의 크기 == 사용하는 어휘집합의 크기
  - 일반적으로 어휘 집합의 크기는 큰 값이기 때문에 단어 표현이 Sparse 해짐
    - 즉, 어휘집합의 크기가 10,000인 경우 하나의 데이터를 표현하기 위해 9999개의 0이 들어간 인코딩 행렬이 만들어지게 됨
  
## 자연어 처리를 위한 기초 수학
### Random Variable
확률론에서 특정 사건이 발생할 확률을 나타냄
- 주사위를 던졌을때 1이 나올 확률 : P(x=1) = 1/6

### Joint Probability
여러 개의 사건이 동시에 일어날 확률
- 두 개의 주사위 A, B를 던졌을때 A 주사위는 1, B 주사위는 2가 나올 결합 확률 : P(A=1, B=2)
- 이때 주사위 던지기는 각각의 주사위 던지기가 다른 주사위 던지기 결과에 영향을 끼치지 않음. 이를 두 사건이 **독립(independent)** 이라 표현
- 두 사건이 독립일 경우 다음의 조건을 만족
  - P(A,B)=P(A)P(B)

### Conditional Probability
특정 사건이 발생했을 때 다른 사건이 발생할 확률 : P(B|A)=P(A,B)/P(A)
- 조건부 확률은 자연어 처리에서 광범위하게 사용됨

### Maximum Likelihood Estimation
어떤 현상이 발생했을 때 그 현상이 발생할 확률이 가장 높은 Likelihood(가능도)를 추정하는 방법론
- 예를 들어 어떤 주머니에서 3개의 공을 꺼냈을 때 빨간공 2개, 초록색공 1개가 나왔다면 이 공을 꺼낸 주머니에서 빨간색공과 초록색공 몇개가 있어야만 이런 현상이 발생할 확률이 가장 높은지를 추정하는 과정이 MLE임
- MLE는 머신러닝과 자연어처리 분야에 광범위하게 사용됨

## NLTK 라이브러리 & NLTK를 이용한 토크나이징
실습 [```자연어처리_기초_토크나이징과_원핫인코딩.ipynb```](https://github.com/EHOia/S_AI_NLP/blob/main/자연어처리_기초_토크나이징과_원핫인코딩.ipynb)
## 자연어 처리를 위해 학습해야 하는 Python 라이브러리
### Pandas, Numpy, HuggingFace
